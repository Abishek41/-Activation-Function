{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b20700b",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "Ans: An activation function is a mathematical function that introduces non-linearity into the network's decision-making process. It is applied to the weighted sum of the inputs of a neuron.\n",
    "                        \n",
    "Without an activation function, a neural network would be limited to representing linear transformations of the input data, making it less powerful and restricting its ability to learn and approximate complex functions.\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a1fae",
   "metadata": {},
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "Ans: There are three different types of activation function:\n",
    "\n",
    "a. Sigmoid function.\n",
    "\n",
    "b. Tanh\n",
    "\n",
    "c. Relu\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a20229",
   "metadata": {},
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "Ans: Activation function plays a important role in training process.\n",
    "\n",
    "a. Non-linearity and Representation.\n",
    "\n",
    "b. Gradient Flow and Vanishing/Exploding Gradients.\n",
    "\n",
    "c. The choice of activation function can impact the speed of convergence during training.\n",
    "\n",
    "d. Activation functions have different output ranges, which can impact the suitability for specific tasks. \n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030887c",
   "metadata": {},
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "Ans: In an artificial neural network (ANN), the sigmoid activation function is applied to the weighted sum of inputs of a neuron, also known as the activation. It introduces non-linearity and transforms the input into a value between 0 and 1 using the sigmoid function.\n",
    "\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "where 'x' represents the input to the neuron.\n",
    "\n",
    "#### Advantage:\n",
    "\n",
    "a. Smoothness and differentiability.\n",
    "\n",
    "b. Output range and probability interpretation.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "a. Vanishing gradients.\n",
    "\n",
    "b. Computationally expensive.\n",
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b7bea",
   "metadata": {},
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "Ans: The Rectified Linear Unit (ReLU) activation function is a popular non-linear activation function used in artificial neural networks. It is defined as follows:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "In other words, for any given input 'x', the ReLU function returns 'x' if 'x' is greater than 0, and returns 0 otherwise.\n",
    "\n",
    "#### It differ from the sigmoid function:\n",
    "a. The range for relu starts from 0 to infinity.\n",
    "\n",
    "b.  ReLU is more sparse and introduces sparsity in activation patterns by zeroing out negative inputs.\n",
    "\n",
    "c. Avoiding vanishing gradients.\n",
    "\n",
    "d. Computational efficiency.\n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800e095",
   "metadata": {},
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Ans: a. Avoidance of vanishing gradients.\n",
    "\n",
    "b. Computationally efficient.\n",
    "\n",
    "c. Avoidance of saturation.\n",
    "\n",
    "d. Better initialization and convergence.\n",
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc0668",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "Ans: The Leaky ReLU is a variant of the ReLU activation function that introduces a small slope to the negative part of the function.\n",
    "It addresses the vanishing gradient problem by allowing a small gradient to pass through the activation function for negative inputs.\n",
    "The non-zero slope in the negative region helps propagate gradients and update the weights during training, preventing neurons from becoming completely inactive.\n",
    "By maintaining some gradient flow for negative inputs, the Leaky ReLU promotes better learning and mitigates the dying ReLU problem in deep neural networks.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003f020",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "Ans: The softmax activation function is used in the output layer of a neural network for multi-class classification problems. It takes a vector of real-valued inputs and normalizes them into a probability distribution over multiple classes. The purpose of the softmax function is to provide a way to interpret the network's output as class probabilities.\n",
    "\n",
    "Mathematically, the softmax function transforms a vector of logits (unnormalized scores) into a probability distribution:\n",
    "\n",
    "softmax(x[i]) = exp(x[i]) / (sum(exp(x[j])) for j in range(num_classes))\n",
    "\n",
    "The softmax activation function is commonly used in multi-class classification tasks where the goal is to assign an input to one of several mutually exclusive classes. It is widely used in problems such as image classification, sentiment analysis, natural language processing tasks like text categorization, and other tasks that involve multi-class classification.\n",
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1de2c",
   "metadata": {},
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "Ans: The hyperbolic tangent (tanh) activation function is a widely used activation function in neural networks. It is a non-linear function that maps the input to a value between -1 and 1, similar to the sigmoid function.\n",
    "\n",
    "Mathematically, the tanh activation function is defined as:\n",
    "\n",
    "tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "\n",
    "#### Comparison between tanh and sigmoid activation functions:\n",
    "\n",
    "a. Output range.\n",
    "\n",
    "b. Saturation behavior.\n",
    "\n",
    "c. Computational complexity.\n",
    "*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
